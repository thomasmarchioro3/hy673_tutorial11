{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from unet import Unet\n",
    "from scheduler import get_schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "epochs = 20  # If training takes too long you can reduce the number of epochs or increase the batch size\n",
    "batch_size = 64\n",
    "lr = 1e-5\n",
    "\n",
    "n_T = 1000\n",
    "betas = [1e-4, 0.02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "dataset = MNIST(\"./data\", train=True, download=True, transform=transform,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "unet = Unet(n_features=128).to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optim = Adam(unet.parameters(), lr=lr)\n",
    "\n",
    "# pre-compute schedules\n",
    "schedules = get_schedules(betas[0], betas[1], n_T)\n",
    "schedules = {key: val.to(device) for key, val in schedules.items()}  # add all tensors on device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory if it does not exist\n",
    "if not os.path.isdir('results'):\n",
    "    os.makedirs('results')\n",
    "\n",
    "if not os.path.isdir('saved_models'):\n",
    "    os.makedirs('saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, n_T, n_samples, sample_shape, device, schedules):\n",
    "\n",
    "    # Step 1\n",
    "    x_T = torch.randn(n_samples, *sample_shape).to(device)\n",
    "    ones = torch.ones(n_samples).to(device)\n",
    "\n",
    "    # Step 2\n",
    "    x_i = x_T\n",
    "    for i in tqdm(range(n_T, 0, -1)):\n",
    "        # Step 3\n",
    "        z = torch.randn(n_samples, *sample_shape).to(device) if i > 1 else 0\n",
    "        # Step 4\n",
    "        t = (i / n_T)*ones\n",
    "        eps = model(x_i, t)\n",
    "        eps = eps.clone().detach()\n",
    "        v = schedules[\"one_over_sqrt_a\"][i] * (x_i - eps * schedules[\"inv_alpha_over_sqrt_inv_abar\"][i])\n",
    "        x_i = v + schedules[\"sqrt_beta\"][i] * z\n",
    "\n",
    "\n",
    "    # Step 6\n",
    "    x = x_i\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.3512: 100%|██████████| 938/938 [04:43<00:00,  3.31it/s]\n",
      "100%|██████████| 1000/1000 [00:13<00:00, 72.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.2321: 100%|██████████| 938/938 [04:43<00:00,  3.31it/s]\n",
      "100%|██████████| 1000/1000 [00:14<00:00, 69.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.1840: 100%|██████████| 938/938 [04:43<00:00,  3.31it/s]\n",
      "100%|██████████| 1000/1000 [00:14<00:00, 70.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.1566: 100%|██████████| 938/938 [04:43<00:00,  3.31it/s]\n",
      "100%|██████████| 1000/1000 [00:14<00:00, 71.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.1386: 100%|██████████| 938/938 [04:43<00:00,  3.31it/s]\n",
      "100%|██████████| 1000/1000 [00:13<00:00, 73.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.1257: 100%|██████████| 938/938 [04:43<00:00,  3.30it/s]\n",
      "100%|██████████| 1000/1000 [00:14<00:00, 71.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.1159: 100%|██████████| 938/938 [04:42<00:00,  3.32it/s]\n",
      "100%|██████████| 1000/1000 [00:13<00:00, 73.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 : \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg loss: 0.1143:  20%|█▉        | 186/938 [00:56<03:48,  3.29it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(eps_hat, eps)\n\u001b[1;32m     29\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 30\u001b[0m losses\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     31\u001b[0m pbar\u001b[39m.\u001b[39mset_description(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAvg loss: \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mmean(losses)\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m optim\u001b[39m.\u001b[39mstep()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1} : \")\n",
    "\n",
    "    # Set unet in training mode\n",
    "    unet.train()\n",
    "\n",
    "    pbar = tqdm(dataloader)\n",
    "    for x, _ in pbar:\n",
    "        x = x.to(device)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "\n",
    "        # Step 3\n",
    "        timesteps = torch.randint(1, n_T + 1, (x.shape[0],)).to(device)\n",
    "\n",
    "        # Step 4\n",
    "        eps = torch.randn_like(x)\n",
    "\n",
    "        # Step 5\n",
    "        optim.zero_grad()\n",
    "\n",
    "        x_t = schedules[\"sqrt_abar\"][timesteps, None, None, None] * x + schedules[\"sqrt_inv_abar\"][timesteps, None, None, None] * eps\n",
    "        t = timesteps/n_T\n",
    "        eps_hat = unet(x_t, t)\n",
    "\n",
    "\n",
    "        loss = loss_fn(eps_hat, eps)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        avg_loss = np.asarray(losses)[-10:].mean()\n",
    "        pbar.set_description(f\"Moving average loss: {avg_loss:.4f}\")\n",
    "        optim.step()\n",
    "\n",
    "    # Set unet in eval mode\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        x_hat = sample(unet, n_T, 8, (1, 28, 28), device, schedules)\n",
    "        grid = make_grid(x_hat, normalize=True, value_range=(-1, 1), nrow=2)\n",
    "\n",
    "        # IMPORTANT: the grid contains \n",
    "        save_image(grid, f\"results/sample_mnist{epoch+1}.png\")\n",
    "\n",
    "        # save model\n",
    "        torch.save(unet.state_dict(), f\"saved_models/unet_mnist.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
