{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UNet\n",
    "\n",
    "Diffusion models often follow UNet-like architectures, which empirically provide the best results.\n",
    "UNet was originally proposed as a model for image segmentation, but it also has proven to be effective in diffusion models based on denoising, due to their ability to capture different levels of local and global features of an image.\n",
    "\n",
    "UNet-like architectures are designed according to the following principles."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder-decoder structure\n",
    "\n",
    "The encoder captures the context and extracts high-level features from the input image, while the decoder reconstructs the segmented output by upsampling and combining the features from the encoder. This structure allows the network to learn both local and global information.\n",
    "\n",
    "IMPORTANT: Do not get confused with variational autoencoders! In diffusion models, the encoder's output (a.k.a. latent variable) does not serve any specific purpose, except for one that will be discussed later. The encoder-decoder structure is mainly use to extract different types of features. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contracting path (Encoder)\n",
    "\n",
    "In practice, starting from an image, the encoder gradually increases the number of channels and gradually decreases its dimensionality.\n",
    "\n",
    "This is called a <i>contracting path</i>. For example, the tensor dimensionality may undergo the following transformations, going from the input to the latent space:\n",
    "\n",
    "Input: 1x28x28 --> Intermediate 1: 64x14x14 --> Intermediate 2: 128x7x7 --> Intermediate 3: 256x3x3 --> Latent: 256x1x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This ConvBlock is the elementary block of UNet (for both encoder and decoder)\n",
    "# ConvBlock preserves dimensionality\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, 1, 1),\n",
    "            nn.GroupNorm(8, out_channels),  # works much better than BatchNormalization for this model\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "    \n",
    "# Elementary encoder block\n",
    "class UnetDown(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, downscale=2):\n",
    "        super(UnetDown, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            ConvBlock(in_channels, out_channels), \n",
    "            nn.MaxPool2d(downscale)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        return self.model(x)\n",
    "    \n",
    "# Encoder network\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.initial_features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1),\n",
    "            nn.GroupNorm(8, 64),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.down1 = UnetDown(64, 64, downscale=2)\n",
    "        self.down2 = UnetDown(64, 128, downscale=2)\n",
    "        self.down3 = UnetDown(128, 128, downscale=2)\n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.AvgPool2d(3), \n",
    "            nn.ReLU())\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x_f = self.initial_features(x)\n",
    "        d1 = self.down1(x_f)\n",
    "        d2 = self.down2(d1)\n",
    "        d3 = self.down3(d2)\n",
    "        latent = self.down4(d3)\n",
    "        return x_f, d1, d2, d3, latent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "dataset = MNIST(\"./data\", train=True, download=True, transform=transform,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "x, _ = next(iter(dataloader))\n",
    "x = x.view(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([32, 1, 28, 28])\n",
      "Down 1 shape: torch.Size([32, 64, 14, 14])\n",
      "Down 2 shape: torch.Size([32, 128, 7, 7])\n",
      "Down 3 shape: torch.Size([32, 128, 3, 3])\n",
      "Latent shape: torch.Size([32, 128, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder()\n",
    "\n",
    "x_f, d1, d2, d3, latent = encoder(x)\n",
    "\n",
    "print(\"Input shape:\", x.shape)\n",
    "print(\"Down 1 shape:\", d1.shape)\n",
    "print(\"Down 2 shape:\", d2.shape)\n",
    "print(\"Down 3 shape:\", d3.shape)\n",
    "print(\"Latent shape:\", latent.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding path and skip connections (decoder)\n",
    "\n",
    "In denoising model, the decoder still has a \"reverse\" structure compared to the encoder. However, its purpose is to predict the next reconstruction step, rather than the entire image. Additionally, in the case of UNet, the encoder and the decoder are not separated, and we can take advantage of that.\n",
    "\n",
    "In particular, we feed intermediate output of the encoder to the decoder layers. These are called \"skip connections\", since the intermediate outputs 'skip' part of the model.\n",
    "\n",
    "    Initial Layer ----------------------- Tensor: x_f ---------------------------> Out Layer\n",
    "            |                                                                          Ʌ\n",
    "            V                                                                          |\n",
    "        Enc Layer 1 --------------------- Tensor: d1 ------------------------> Dec Layer 4\n",
    "                |                                                                Ʌ\n",
    "                V                                                                |\n",
    "            Enc Layer 2 ----------------- Tensor: d2 ------------------> Dec Layer 3\n",
    "                    |                                                        Ʌ\n",
    "                    V                                                        |\n",
    "                Enc Layer 3 ------------- Tensor: d3 --------------> Dec Layer 2\n",
    "                        |                                               Ʌ\n",
    "                        V                                               |\n",
    "                    Enc Layer 4 --------- Tensor: latent ----------> Dec Layer 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UnetUp aims at reverting UnetDown, but also uses skip connections\n",
    "# In some cases, output padding is needed (e.g., on the second layer,\n",
    "# with upscale 2 we get Cx3x3--> C'x6x6, so we need extra_dim 1 to get C'x7x7)\n",
    "class UnetUp(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, upscale=2, extra_dim=0):\n",
    "        super(UnetUp, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, upscale, upscale, output_padding=extra_dim),\n",
    "            ConvBlock(out_channels, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "        x = torch.cat((x, skip), 1)\n",
    "        x = self.model(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "# Decoder network\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # first decoder layer does not have any skip connections\n",
    "        self.up1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(128, 128, 3, 3),\n",
    "            nn.GroupNorm(8, 128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        self.up2 = UnetUp(256, 128, upscale=2, extra_dim=1) \n",
    "        self.up3 = UnetUp(256, 64, upscale=2)\n",
    "        self.up4 = UnetUp(128, 64, upscale=2)\n",
    "        self.out = nn.Conv2d(128, 1, 3, 1, 1)\n",
    "\n",
    "    def forward(self, latent, d3, d2, d1, x_f):\n",
    "\n",
    "        u1 = self.up1(latent)\n",
    "        u2 = self.up2(u1, d3)\n",
    "        u3 = self.up3(u2, d2)\n",
    "        u4 = self.up4(u3, d1)\n",
    "        eps_hat = self.out(torch.cat([u4, x_f], dim=1))\n",
    "        return u1, u2, u3, u4, eps_hat\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latent shape: torch.Size([32, 128, 1, 1])\n",
      "Up 1 shape: torch.Size([32, 128, 3, 3])\n",
      "Up 2 shape: torch.Size([32, 128, 7, 7])\n",
      "Up 3 shape: torch.Size([32, 64, 14, 14])\n",
      "Up 4 shape: torch.Size([32, 64, 28, 28])\n",
      "Output shape: torch.Size([32, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder()\n",
    "\n",
    "u1, u2, u3, u4, eps_hat = decoder(latent, d3, d2, d1, x_f)\n",
    "\n",
    "print(\"Latent shape:\", latent.shape)\n",
    "print(\"Up 1 shape:\", u1.shape)\n",
    "print(\"Up 2 shape:\", u2.shape)\n",
    "print(\"Up 3 shape:\", u3.shape)\n",
    "print(\"Up 4 shape:\", u4.shape)\n",
    "print(\"Output shape:\", eps_hat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clear variables to limit memory consumption\n",
    "del u1, u2, u3, eps_hat "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding temporal information\n",
    "\n",
    "In denoising-based diffusion models, the generation occurs through multiple denoising steps. To enhance the quality of generation, the model needs to be informed about the current processing step $t$.\n",
    "\n",
    "This can be done by encoding the information into additional 1-dimensional channels that get summed to the latent variable. These are called <i>time embeddings</i> and, clearly, should have the same dimensionality as the latent variable. Before being processed by the time embedding, $t$ should be normalized w.r.t. the max number $T$ of steps.\n",
    "\n",
    "There is also the possibility to concatenate time embeddings as extra channels, but for MNIST summing them is perfectly fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time embedding layer\n",
    "class TimeEmbedding(nn.Module):\n",
    "    def __init__(self, embedding_dim):\n",
    "        super(TimeEmbedding, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.lin1 = nn.Linear(1, 32, bias=False)\n",
    "        self.lin2 = nn.Linear(32, embedding_dim)\n",
    "\n",
    "    def forward(self, ts):\n",
    "        ts = ts.view(-1, 1)\n",
    "        temb = torch.sin(self.lin1(ts))  # sine activation is common to encode time information\n",
    "        temb = self.lin2(temb)\n",
    "        temb = temb.view(-1, self.embedding_dim, 1, 1)  # add dummy channels to sum with z\n",
    "        return temb\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_embedding_layer  = TimeEmbedding(128)\n",
    "\n",
    "n_T = 1000  # number of time steps used for reconstruction \n",
    "timesteps = torch.randint(1, n_T + 1, (x.shape[0],))\n",
    "t = timesteps/n_T\n",
    "\n",
    "temb = time_embedding_layer(t)\n",
    "\n",
    "latent_temb = latent + temb\n",
    "\n",
    "u1, u2, u3, u4, eps_hat = decoder(latent_temb, d3, d2, d1, x_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clear variables to limit memory consumption\n",
    "del u1, u2, u3, u4, eps_hat "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UNet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall UNet model\n",
    "class Unet(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "        self.encoder = Encoder()\n",
    "        self.decoder = Decoder()\n",
    "        self.time_embedding_layer = TimeEmbedding(128)\n",
    "    \n",
    "    def forward(self, x, t):\n",
    "\n",
    "        x_f, d1, d2, d3, latent = self.encoder(x)\n",
    "        temb = self.time_embedding_layer(t)\n",
    "        latent_temb = latent + temb\n",
    "        _, _, _, _, eps_hat = self.decoder(latent_temb, d3, d2, d1, x_f)  # intermediate decoder steps are not needed\n",
    "\n",
    "        return eps_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unet = Unet()\n",
    "\n",
    "eps_hat = unet(x, t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
