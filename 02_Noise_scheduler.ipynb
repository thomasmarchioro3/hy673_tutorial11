{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noise scheduler"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When training diffusion models, we gradually apply noise to our image, until the output is normally distributed with $\\mathcal{N}(0, 1)$.\n",
    "The model is trained to invert this process and gradually reconstruct an image from noise.\n",
    "\n",
    "![brokenimage](/home/marchiorot/Desktop/HY673/Tutorial11/figures/diff_intuition.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original image is $x_0$, while gaussian noise is $x_T$ where $T$ is the number of reconstruction steps that the model will use for sampling (hyperparameter).\n",
    "\n",
    "During training, we apply $t$ steps of noise to an image in our training data, with $t$ chosen uniformly in $\\{1,2, \\dots, T\\}$.\n",
    "Core idea:\n",
    "\n",
    "- Noise gets applied according to $x_{t}\\sim \\mathcal{N}(\\sqrt{1-\\beta_t} x_{t-1}, \\beta_t)$\n",
    "- The model's objective is to go back from $x_{t}$ to $x_{t-1}$ (predict one single noise step $\\epsilon_t$)\n",
    "\n",
    "So the loss can be, for example, the MSE between the true $\\epsilon_t$ and the model's prediction $\\hat{\\epsilon}_{t}$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem\n",
    "\n",
    "Applying noise to get from $x_0$ to $x_t$ is normally an iterative procedure that requires sampling noise from a Gaussian distribution $t$ times, and averaging this noise with the image. \n",
    "If we actually did this process in multiple iterations, training large diffusion models would be a very long procedure, sometimes infeasible.\n",
    "\n",
    "Luckily, some smart mathematician came up with a method to pre-compute noise with one single step.\n",
    "\n",
    "This method computes the noise parameters at step $t$ which are the $\\bar{\\alpha}_{t}$ coefficients in the algorithms below."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find more details in the slides or reading the original paper: https://arxiv.org/abs/2006.11239"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trick to apply multiple noise steps\n",
    "\n",
    "During training (see loss function below), we need to go from $x_0$ to a specific $x_t$.\n",
    "\n",
    "A single noise step gets applied according to $x_{t}\\sim \\mathcal{N}(\\sqrt{1-\\beta_t} x_{t-1}, \\beta_t)$. $\\beta_1$ and $\\beta_T$ are hyperparameters, the other $\\beta_t$ values are linearly spaced within the range $[\\beta_1, \\beta_T]$.\n",
    "\n",
    "<b>Problem</b>: Producing $x_t$'s sequentially would be computationally expensive.\n",
    "\n",
    "<b>Trick</b>: There is a formula to get $x_t$ directly from $x_0$\n",
    "$$\n",
    "x_t \\sim \\mathcal{N} (\\sqrt{\\bar{\\alpha}_t} x_0, 1-\\bar{\\alpha}_t)\n",
    "$$\n",
    "with\n",
    "$$\n",
    "\\alpha_t = 1 - \\beta_t\n",
    "$$\n",
    "and\n",
    "$$\n",
    "\\bar{\\alpha}_t = \\prod_{s=1}^{t} \\alpha_s \\ \\ \\text{(Cumulative product of $\\alpha$'s)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from unet import Unet\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_schedules(beta_1, beta_T, n_T):\n",
    "    \"\"\"\n",
    "    Linear scheduler. \n",
    "    Useful to pre-compute all the parameters (even fractions, square roots, etc).\n",
    "    \"\"\"\n",
    "\n",
    "    beta_t = (beta_T - beta_1) * torch.arange(0, n_T + 1, dtype=torch.float32) / n_T + beta_1\n",
    "    sqrt_beta_t = torch.sqrt(beta_t)\n",
    "    alpha_t = 1 - beta_t\n",
    "    log_alpha_t = torch.log(alpha_t)\n",
    "    alphabar_t = torch.cumsum(log_alpha_t, dim=0).exp()\n",
    "\n",
    "    sqrt_abar = torch.sqrt(alphabar_t)\n",
    "    one_over_sqrt_a = 1 / torch.sqrt(alpha_t)\n",
    "\n",
    "    sqrt_inv_abar = torch.sqrt(1 - alphabar_t)\n",
    "    inv_abar_over_sqrt_inv_abar = (1 - alpha_t) / sqrt_inv_abar\n",
    "\n",
    "    return {\n",
    "        \"alpha\": alpha_t,  # \\alpha_t\n",
    "        \"one_over_sqrt_a\": one_over_sqrt_a,  # 1/\\sqrt{\\alpha_t}\n",
    "        \"sqrt_beta\": sqrt_beta_t,  # \\sqrt{\\beta_t}\n",
    "        \"alphabar\": alphabar_t,  # \\bar{\\alpha_t}\n",
    "        \"sqrt_abar\": sqrt_abar,  # \\sqrt{\\bar{\\alpha_t}}\n",
    "        \"sqrt_inv_abar\": sqrt_inv_abar,  # \\sqrt{1-\\bar{\\alpha_t}}\n",
    "        \"inv_alpha_over_sqrt_inv_abar\": inv_abar_over_sqrt_inv_abar,  # (1-\\alpha_t)/\\sqrt{1-\\bar{\\alpha_t}}\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_T = 1000\n",
    "betas = [1e-4, 0.02]\n",
    "\n",
    "schedules = get_schedules(betas[0], betas[1], n_T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![brokenfig](/home/marchiorot/Desktop/HY673/Tutorial11/figures/training_algo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset, model, loss function, and optimizer\n",
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "dataset = MNIST(\"./data\", train=True, download=True, transform=transform,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "unet = Unet()\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "optim = torch.optim.Adam(unet.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "x, _ = next(iter(dataloader))\n",
    "x = x.view(-1, 1, 28, 28)\n",
    "\n",
    "# Step 3\n",
    "timesteps = torch.randint(1, n_T + 1, (x.shape[0],))\n",
    "\n",
    "# Step 4\n",
    "eps = torch.randn_like(x)\n",
    "\n",
    "# Step 5\n",
    "optim.zero_grad()\n",
    "\n",
    "x_t = schedules[\"sqrt_abar\"][timesteps, None, None, None] * x + schedules[\"sqrt_inv_abar\"][timesteps, None, None, None] * eps\n",
    "t = timesteps/n_T\n",
    "eps_hat = unet(x_t, t)\n",
    "loss = loss_fn(eps_hat, eps)\n",
    "loss.backward()\n",
    "\n",
    "optim.step()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![brokenfig](/home/marchiorot/Desktop/HY673/Tutorial11/figures/sampling_algo.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, n_T, n_samples, sample_shape, schedules):\n",
    "\n",
    "    # Step 1\n",
    "    x_T = torch.randn(n_samples, *sample_shape)\n",
    "\n",
    "    # Step 2\n",
    "    x_i = x_T\n",
    "    for i in tqdm(range(n_T, 0, -1)):\n",
    "        # Step 3\n",
    "        z = torch.randn(n_samples, *sample_shape) if i > 1 else 0\n",
    "\n",
    "        # Step 4\n",
    "        ts = torch.tensor(i / n_T).repeat(n_samples,)\n",
    "        eps = model(x_i, ts)\n",
    "        x_i = schedules[\"one_over_sqrt_a\"][i] * (x_i - eps * schedules[\"inv_alpha_over_sqrt_inv_abar\"][i]) + schedules[\"sqrt_beta\"][i] * z\n",
    "\n",
    "\n",
    "    # Step 6\n",
    "    x = x_i\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:14<00:00, 69.33it/s]\n"
     ]
    }
   ],
   "source": [
    "x = sample(unet, n_T, 8, (1, 28, 28), schedules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
