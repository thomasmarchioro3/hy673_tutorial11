{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image, make_grid\n",
    "\n",
    "from unet import Unet\n",
    "from scheduler import get_schedules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "\n",
    "epochs = 1\n",
    "batch_size = 64\n",
    "lr = 1e-5\n",
    "\n",
    "n_T = 1000\n",
    "betas = [1e-4, 0.02]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dataset\n",
    "transform = Compose([ToTensor(), Normalize((0.5,), (0.5,))])\n",
    "dataset = MNIST(\"./data\", train=True, download=True, transform=transform,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "unet = Unet().to(device)\n",
    "loss_fn = nn.MSELoss()\n",
    "optim = Adam(unet.parameters(), lr=lr)\n",
    "\n",
    "# pre-compute schedules\n",
    "schedules = get_schedules(betas[0], betas[1], n_T)\n",
    "schedules = {key: val.to(device) for key, val in schedules.items()}  # add all tensors on device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory if it does not exist\n",
    "if not os.path.isdir('results'):\n",
    "    os.makedirs('results')\n",
    "\n",
    "if not os.path.isdir('saved_models'):\n",
    "    os.makedirs('saved_models')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling function (used to log the results)\n",
    "def sample(model, n_T, n_samples, sample_shape, schedules):\n",
    "\n",
    "    # Step 1\n",
    "    x_T = torch.randn(n_samples, *sample_shape).to(device)\n",
    "\n",
    "    # Step 2\n",
    "    x_i = x_T\n",
    "    for i in tqdm(range(n_T, 0, -1)):\n",
    "        # Step 3\n",
    "        z = torch.randn(n_samples, *sample_shape).to(device) if i > 1 else 0\n",
    "        # Step 4\n",
    "        ts = torch.tensor(i / n_T).repeat(n_samples,).to(device)\n",
    "        eps = model(x_i, ts)\n",
    "        x_i = schedules[\"one_over_sqrt_a\"][i] * (x_i - eps * schedules[\"inv_alpha_over_sqrt_inv_abar\"][i]) + schedules[\"sqrt_beta\"][i] * z\n",
    "\n",
    "\n",
    "    # Step 6\n",
    "    x = x_i\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "losses = []\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch} : \")\n",
    "\n",
    "    # Set unet in training mode\n",
    "    unet.train()\n",
    "\n",
    "    pbar = tqdm(dataloader)\n",
    "    for x, _ in pbar:\n",
    "        \n",
    "        x = x.to(device)\n",
    "        x = x.view(-1, 1, 28, 28)\n",
    "\n",
    "        # Step 3\n",
    "        timesteps = torch.randint(1, n_T + 1, (x.shape[0],)).to(device)\n",
    "\n",
    "        # Step 4\n",
    "        eps = torch.randn_like(x)\n",
    "\n",
    "        # Step 5\n",
    "        optim.zero_grad()\n",
    "\n",
    "        x_t = schedules[\"sqrt_abar\"][timesteps, None, None, None] * x + schedules[\"sqrt_inv_abar\"][timesteps, None, None, None] * eps\n",
    "        t = timesteps/n_T\n",
    "        eps_hat = unet(x_t, t)\n",
    "\n",
    "\n",
    "        loss = loss_fn(eps_hat, eps)\n",
    "        loss.backward()\n",
    "        losses.append(loss.item())\n",
    "        pbar.set_description(f\"Avg loss: {np.mean(losses):.4f}\")\n",
    "        optim.step()\n",
    "\n",
    "    # Set unet in eval mode\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        x_hat = sample(unet, n_T, 8, (1, 28, 28), device, schedules)\n",
    "        x_comp = torch.cat([x_hat, x[:8]], dim=0)  # compare original and reconstructed examples\n",
    "        grid = make_grid(x_comp, normalize=True, value_range=(-1, 1), nrow=4)\n",
    "        save_image(grid, f\"results/sample_mnist{epoch}.png\")\n",
    "\n",
    "        # save model\n",
    "        torch.save(unet.state_dict(), f\"saved_models/unet_mnist.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hy673",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
